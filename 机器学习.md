---
title: 机器学习
tags:
---
# 机器学习通用工作流程
定义问题:分类\回归
获取数据
分析数据
准备数据
研究模型
微调模型
实验验证与模型评估
启动监视维护模型


# 一、机器学习的概述
## 1. 机器学习的五大流派
机器学习大致出现五大流派：符号主义、贝叶斯、联结主义、进化主义、行为类推主义。
- 符号主意：通过符号和公式表达事物的规律，代表模型有专家系统、决策树、逻辑回归等。
- 贝叶斯主义：代表模型贝叶斯网络、朴素贝叶斯分类器、马尔可夫链蒙特卡洛（MCMC）
- 联结主义：通过相互连接的神经元表达知识，代表模型有人工神经网络（ANN）、卷积神经网络（CNN）、递归神经网络（RNN）等深度学习模型。
- 进化主意：代表模型遗传算法（Genetic Algorithm）、遗传编程（Genetic Programming）、进化策略（Evolutionary Strategies）等。
- 行为类推主意：代表模型案例推理（Case-Based Reasoning, CBR）、K近邻算法（k-Nearest Neighbors, k-NN）


## 2. 机器学习算法的分类
- 监督学习：监督学习的任务类型包括，回归分析、统计分析和分类
- 非监督学习： 常见的非监督学习任务类型包括，
- 半监督学习：

# 二、机器学习模型的评估与选择
## 1. 过拟合和欠拟合问题
过拟合是在训练集上表现优秀，但是在验证集上表现较差。  
欠拟合在所有数据集上都表现得不好。  
过拟合的一个例子是，假定在CV任务中，在训练集和测试集上，某个类别a都有椒盐噪声。在验证集上，便将所有有噪声的数据都识别为了类别a。

# 三、决策树（decision tree）
### 3.1 决策树的概述：
决策树是一个树形结构的模型，每个分支节点是一个判断，每个叶节点是一个分类的类型。  
决策树的训练过程是，将输入数据自上而下通过每个节点，如果在一个节点中，所有数据都属于同一个类别，那么这个节点就是一个叶节点。如果数据属于不同的类别，那么就将这个节点分裂。  

### 3.2 决策树的划分选择
决策树中，我们希望的情况下，越上层的节点对数据的区分度应该越大，越下层的节点对数据的区分度应该越小。为了将区分程度大的特征放在树的上层位置，当前研究人员提出了几种数据划分策略，这些策略主要研究特征对分类结果的影响。

- 信息熵
信息熵的公式为  
\[
H(X) = - \sum_{i=1}^{n} P(x_i) \log_2 P(x_i)
\]
显然， 可以不严谨地讲，对于划分平均程度相同的分类方式，划分的类别越多，信息熵越大。
对于分类类别数相同的划分，划分的越平均信息熵越大。

- 信息增益
信息增益的公式为
\[
H(D | A) = \sum_{v \in \text{Values}(A)} \frac{|D_v|}{|D|} H(D_v)
\]
信息增益是进行一次划分之后，信息熵的减少程度。

- 基尼指数  
基尼指数是指随机选取两个样本，分属于不同类别的概率。
基尼指数的公式为：

\[
Gini = 1 - \sum_{k=1}^{K} p_k^2
\]
这个公式的含义是，使用容斥原理，用1减去两个样本属于相同类别的概率。

- 信息增益和基尼指数的比较
大多数情况下，信息增益和基尼指数得出的划分策略是相同的，但是基尼指数的计算较为简单。


### 3.3 决策树的剪枝
稍加思考可知，对于某个数据集D，只要决策树的深度和宽度足够大，那么一定可以在训练数据集上达到100%的准确率，但是这种情况会导致树的空间复杂度过大。此外，由于训练集不可能穷尽所有的情况，因此，还会导致过拟合。  
决策树的剪枝分为预剪枝和后剪枝。其中，预剪枝是指在决策树生成的时候进行剪枝。后剪枝是指在决策树生成之后进行剪枝。

#### 3.3.1 决策树的预剪枝
决策树的预剪枝包括：最大深度限制、最小样本数限制、最小信息增益/基尼指数限制、最小叶节点样本数限制、基于交叉验证的早停 和 最大叶节点数限制。  
西瓜树上样例所讲的是基于交叉验证的早停。  

#### 3.3.2 决策树的后剪枝
后剪枝是在决策树构造完成之后进行剪枝，后剪枝也是基于预测的准确率来进行剪枝。与预剪枝不同的是，预剪枝是在自顶而下构造决策树时进行的剪枝，后剪枝是自底而上的剪枝。

# 支持向量机 
支持向量机是一个用于解决分类问题的模型。支持向量机是通过一个超平面将样本空间分成两个子空间。

# 神经网络
这一部分请见深度学习。

# 七、贝叶斯分类器
## 贝叶斯公式
\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
\]
其中：
- \( P(A|B) \) 是在事件 \( B \) 发生的情况下，事件 \( A \) 发生的后验概率。
- \( P(B|A) \) 是在事件 \( A \) 发生的情况下，事件 \( B \) 发生的条件概率。
- \( P(A) \) 是事件 \( A \) 的先验概率。
- \( P(B) \) 是事件 \( B \) 的边缘概率（归一化常数）。

## 朴素贝叶斯分类器
\[
P(c | \mathbf{x}) = \frac{P(c) P(\mathbf{x} | c)}{P(\mathbf{x})} = \frac{P(c)}{P(\mathbf{x})} \prod_{i=1}^{d} P(x_i | c)
\]

其中：
- \( P(c | \mathbf{x}) \) 是类别 \( c \) 的后验概率。
- \( P(c) \) 是类别 \( c \) 的先验概率。
- \( P(\mathbf{x} | c) \) 是在给定类别 \( c \) 时，特征向量 \( \mathbf{x} \) 的条件概率。
- \( P(\mathbf{x}) \) 是特征向量 \( \mathbf{x} \) 的边缘概率。
- \( P(x_i | c) \) 是在类别 \( c \) 的条件下，第 \( i \) 个特征 \( x_i \) 的条件概率。
- \( d \) 是特征的数量。

上述公式即为朴素贝叶斯分类器的公式。
其中，第一个等号即位贝叶斯公式。第二个等号即位朴素贝叶斯分类器。

朴素贝叶斯分类器将一个特征上，每个特征点出现的条件概率计算出来，然后做乘法。
**朴素贝叶斯分类器之所以叫做朴素，是因为朴素贝叶斯分类器为了简化计算，认为一个特征向量中，各个特征点是相互独立的。  
例如挑西瓜时，好西瓜一般同时具备响声清脆和纹理较绿的特征，这两个特征的出现具有关联性。但是朴素贝叶斯分布简单地认为这两个特征是独立的。**  
因此，贝叶斯分类器的训练过程就是通过数据分布，计算各种概率。
```cpp
/*
1. 问题描述
我们将这个问题转化为预测“好瓜”和“坏瓜”的问题。假设你有以下三个概率：

某种特征（例如色泽青绿）出现在好瓜中的概率：P(色泽青绿 | 好瓜) = 0.8
好瓜的先验概率：P(好瓜) = 0.5
总的色泽为青绿的西瓜的概率：P(色泽青绿) = 0.4
现在的目标是，在已知西瓜是色泽青绿的情况下，预测它是好瓜的概率，即后验概率 P(好瓜 | 色泽青绿)。

2. 应用贝叶斯定理
根据贝叶斯定理，后验概率的计算公式为：

P(好瓜 | 色泽青绿) = [P(色泽青绿 | 好瓜) * P(好瓜)] / P(色泽青绿)

将已知值代入：

P(好瓜 | 色泽青绿) = (0.8 * 0.5) / 0.4

3. 计算后验概率
P(好瓜 | 色泽青绿) = 0.4 / 0.4 = 1.0

*/
```



# 聚类
聚类是一种无监督学习方法，聚类是将不同的数据分成不同的类别。
### 聚类的评价指标
聚类的评价指标有两个：分别是簇内相似度和簇间相似度。
