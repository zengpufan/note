---
title: 机器学习
tags:
---
# 机器学习通用工作流程
定义问题:分类\回归
获取数据
分析数据
准备数据
研究模型
微调模型
实验验证与模型评估
启动监视维护模型


# 一、机器学习的概述
## 1. 机器学习的五大流派
机器学习大致出现五大流派：符号主义、贝叶斯、联结主义、进化主义、行为类推主义。
- 符号主意：通过符号和公式表达事物的规律，代表模型有专家系统、决策树、逻辑回归等。
- 贝叶斯主义：代表模型贝叶斯网络、朴素贝叶斯分类器、马尔可夫链蒙特卡洛（MCMC）
- 联结主义：通过相互连接的神经元表达知识，代表模型有人工神经网络（ANN）、卷积神经网络（CNN）、递归神经网络（RNN）等深度学习模型。
- 进化主意：代表模型遗传算法（Genetic Algorithm）、遗传编程（Genetic Programming）、进化策略（Evolutionary Strategies）等。
- 行为类推主意：代表模型案例推理（Case-Based Reasoning, CBR）、K近邻算法（k-Nearest Neighbors, k-NN）


## 2. 机器学习算法的分类
- 监督学习：监督学习的任务类型包括，回归分析、统计分析和分类
- 非监督学习： 常见的非监督学习任务类型包括，
- 半监督学习：

# 二、机器学习模型的评估与选择
## 1. 过拟合和欠拟合问题
过拟合是在训练集上表现优秀，但是在验证集上表现较差。  
欠拟合在所有数据集上都表现得不好。  
过拟合的一个例子是，假定在CV任务中，在训练集和测试集上，某个类别a都有椒盐噪声。在验证集上，便将所有有噪声的数据都识别为了类别a。

## 3. 决策树（decision tree）
### 3.1 决策树的概述：
决策树是一个树形结构的模型，每个分支节点是一个判断，每个叶节点是一个分类的类型。  
决策树的训练过程是，将输入数据自上而下通过每个节点，如果在一个节点中，所有数据都属于同一个类别，那么这个节点就是一个叶节点。如果数据属于不同的类别，那么就将这个节点分裂。  

### 3.2 决策树的划分选择
决策树中，我们希望的情况下，越上层的节点对数据的区分度应该越大，越下层的节点对数据的区分度应该越小。为了将区分程度大的特征放在树的上层位置，当前研究人员提出了几种数据划分策略，这些策略主要研究特征对分类结果的影响。

- 信息熵
信息熵的公式为  
\[
H(X) = - \sum_{i=1}^{n} P(x_i) \log_2 P(x_i)
\]
显然， 可以不严谨地讲，对于划分平均程度相同的分类方式，划分的类别越多，信息熵越大。
对于分类类别数相同的划分，划分的越平均信息熵越大。

- 信息增益
信息增益的公式为
\[
H(D | A) = \sum_{v \in \text{Values}(A)} \frac{|D_v|}{|D|} H(D_v)
\]
信息增益是进行一次划分之后，信息熵的减少程度。

- 基尼指数  
基尼指数是指随机选取两个样本，分属于不同类别的概率。
基尼指数的公式为：

\[
Gini = 1 - \sum_{k=1}^{K} p_k^2
\]
这个公式的含义是，使用容斥原理，用1减去两个样本属于相同类别的概率。

- 信息增益和基尼指数的比较
大多数情况下，信息增益和基尼指数得出的划分策略是相同的，但是基尼指数的计算较为简单。


### 3.3 决策树的剪枝
稍加思考可知，对于某个数据集D，只要决策树的深度和宽度足够大，那么一定可以在训练数据集上达到100%的准确率，但是这种情况会导致树的空间复杂度过大。  
决策树的剪枝分为预剪枝和后剪枝。其中，预剪枝是指在决策树生成的时候进行剪枝。后剪枝是指在决策树生成之后进行剪枝。

#### 3.3.1 决策树的预剪枝
决策树的预剪枝包括：最大深度限制、最小样本数限制、最小信息增益/基尼指数限制、最小叶节点样本数限制、基于交叉验证的早停 和 最大叶节点数限制。  
西瓜树上样例所讲的是基于交叉验证的早停。  

#### 3.3.2 决策树的后剪枝
后剪枝是在决策树构造完成之后进行剪枝，后剪枝也是基于预测的准确率来进行剪枝。与预剪枝不同的是，预剪枝是在自顶而下构造决策树时进行的剪枝，后剪枝是自底而上的剪枝。

## 支持向量机 
支持向量机是一个用于解决分类问题的模型。支持向量机是通过一个超平面将样本空间分成两个子空间。

## 神经网络
这一部分请见深度学习。

## 聚类
聚类是一种无监督学习方法，聚类是将不同的数据分成不同的类别。
### 聚类的评价指标
聚类的评价指标有两个：分别是簇内相似度和簇间相似度。
