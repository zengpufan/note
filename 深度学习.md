# vision transformer
![](深度学习/transformer.png)
![](深度学习/vision_transformer.png)
```py
class ViT(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=200, embed_dim=768, depth=12, num_heads=12):
        super(ViT, self).__init__()
        self.num_patches = (img_size // patch_size) ** 2
        self.patch_size = patch_size # patch为[16,16]的矩阵
        self.embed_dim = embed_dim # 每个patch会被转换为一个向量，这个向量的长度就是embed dim
        self.num_classes = num_classes

        # Patch Embedding
        # 将图像分为若干个patch，通过设置卷积的步长，来达到将图像分割的目的。之后通过卷积，将每个patch转换为长度为embed_dim的向量
        self.patch_embed = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

        # Positional Embedding
        # pos_embed初始化为随机数值即可，在训练的过程中会自动学习
        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))
        # cls_token是将每个词向量后面加上一个数值（一维向量），用来表示分类数值
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))

        # Transformer blocks
        self.transformer = nn.Transformer(d_model=embed_dim, nhead=num_heads, num_encoder_layers=depth)

        # Classification head
        self.head = nn.Linear(embed_dim, num_classes)

    def forward(self, x):
        # 1. Patch Embedding
        B = x.shape[0]
        x = self.patch_embed(x)  # [B, embed_dim, H', W']将图像分成小块，H‘和W’代表每行每列的分块数量
        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, embed_dim] 由于H‘和W’无需包含空间关系，因此将其展平

        # 2. Add class token
        # cls_token:[1,1,embed_dim] 
        cls_tokens = self.cls_token.expand(B, -1, -1)
        # cls_tokens: [B,1,embed_dim]
        # 这里一个patch相当于transformer中的一个token，transformer需要一个额外的token来表示类别和全局信息
        x = torch.cat((cls_tokens, x), dim=1)
        # x:[B,num_patches+1,embed_dim]

        # 3. Add positional embedding
        x = x + self.pos_embed

        # 4. Transformer layers
        x = self.transformer(x, x)

        # 5. Classification head (taking the cls token output)
        cls_output = x[:, 0]
        x = self.head(cls_output)

        return x
```

```py
import torch
import torch.nn as nn
import math

# Patch Embedding Layer
class PatchEmbedding(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):
        super(PatchEmbedding, self).__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        self.embed_dim = embed_dim

        # Use a Conv2d layer to flatten and project the patches
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        B, C, H, W = x.shape
        assert H == self.img_size and W == self.img_size, "Input image size doesn't match model."
        
        x = self.proj(x)  # Shape: [B, embed_dim, H/patch_size, W/patch_size]
        x = x.flatten(2)  # Shape: [B, embed_dim, num_patches]
        x = x.transpose(1, 2)  # Shape: [B, num_patches, embed_dim]
        
        return x

# Multi-Head Self-Attention
class MultiHeadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == embed_dim, "Embedding dimension must be divisible by num_heads"

        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        self.scale = math.sqrt(self.head_dim)

    def forward(self, x):
        # x:[B,patch_count+1,embed_dim]
        B, N, C = x.shape
        qkv = self.qkv_proj(x).reshape(B, N, 3, self.num_heads, self.head_dim)
        # qkv:[B,N,3,num_heads,head_dim]
        qkv = qkv.permute(2, 0, 3, 1, 4)  
        # qkv:[3 x B x num_heads x N x head_dim]
        q, k, v = qkv[0], qkv[1], qkv[2]
        # q,k,v:[B,num_heads,N,head_dim]
        attn_weights = (q @ k.transpose(-2, -1)) / self.scale
        # attn_weights:[B,num_heads,N,N]
        # 这一步得到了一个N*N的注意力相似度矩阵，表示每个向量和其他向量的相似度权重，这就是注意力
        attn_weights = attn_weights.softmax(dim=-1)

        attn_output = (attn_weights @ v).transpose(1, 2).reshape(B, N, C)

        return self.out_proj(attn_output)

# Feedforward Network
class FeedForward(nn.Module):
    def __init__(self, embed_dim, hidden_dim, dropout=0.1):
        super(FeedForward, self).__init__()
        self.fc1 = nn.Linear(embed_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.dropout(torch.relu(self.fc1(x)))
        x = self.fc2(x)
        return x

# Transformer Encoder Layer
class TransformerEncoderLayer(nn.Module):
    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0.1):
        super(TransformerEncoderLayer, self).__init__()
        self.attention = MultiHeadAttention(embed_dim, num_heads)
        self.feed_forward = FeedForward(embed_dim, hidden_dim, dropout)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        attn_output = self.attention(x)
        x = self.norm1(x + self.dropout(attn_output))
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        return x

# Vision Transformer (ViT)
class VisionTransformer(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, hidden_dim=3072, dropout=0.1):
        super(VisionTransformer, self).__init__()
        self.num_classes = num_classes
        self.embed_dim = embed_dim
        self.num_patches = (img_size // patch_size) ** 2

        # Patch embedding
        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)

        # CLS token (used for classification)
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))

        # Positional embedding
        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))

        # Transformer encoder layers
        self.encoder_layers = nn.ModuleList([
            TransformerEncoderLayer(embed_dim, num_heads, hidden_dim, dropout)
            for _ in range(depth)
        ])

        # Classification head
        self.head = nn.Linear(embed_dim, num_classes)

        self.dropout = nn.Dropout(dropout)
        self.init_weights()

    def init_weights(self):
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        nn.init.trunc_normal_(self.cls_token, std=0.02)
        nn.init.trunc_normal_(self.head.weight, std=0.02)
        if self.head.bias is not None:
            nn.init.zeros_(self.head.bias)

    def forward(self, x):
        B = x.shape[0]

        # Patch embedding
        x = self.patch_embed(x)  # Shape: [B, num_patches, embed_dim]

        # Add CLS token to the patch embeddings
        cls_tokens = self.cls_token.expand(B, -1, -1)  # Shape: [B, 1, embed_dim]
        x = torch.cat((cls_tokens, x), dim=1)  # Shape: [B, num_patches + 1, embed_dim]

        # Add positional embeddings
        x = x + self.pos_embed

        # Apply Transformer encoder layers
        for layer in self.encoder_layers:
            x = layer(x)

        # Extract the CLS token's final representation
        cls_output = x[:, 0]  # Shape: [B, embed_dim]

        # Apply classification head
        x = self.head(cls_output)  # Shape: [B, num_classes]

        return x

```

# 神经网络基础
## MP神经元模型
MP神经元模型包括三个部分，分别是输入、处理和输出。
- 输入是接收其他神经元传来的信息
- 处理是将神经元的输入和权重相乘
- 输出是将处理后的结果通过激活函数，得到输出
![](./深度学习/MP.png)

## 激活函数
初始的激活函数是阶跃函数，这种函数由于不连续，在反向传播中无法求导，现在已经被淘汰。
常见的激活函数有三种，分别是  
sigmoid，relu，tanh

## 感知机

![](./深度学习/双层感知机.png)
- y = f(w1x1 + w2x2 − θ)
- 感知机的学习：
感知机的可学习参数包括权重W和输出的阈值θ

- 多层感知机
多层感知机在输入层和输出层之间加入了多个隐藏层。

## 反向传播算法


# 卷积神经网络
## 卷基层
- 填充
- 步幅
步幅指每次卷积操作后，卷积核移动的距离
- 感受野
指一次卷积操作可以提取到的图像特征的范围（像素点的个数）
- 多通道卷基层

- 共享卷积核的参数
多个卷积核使用相同的参数，这可以提高训练效率。
## 池化层
池化层就是对图像进行下采样。池化层包括最大池化和平均池化等。

# 经典的卷积神经网络
## AlexNet
- AlexNet通过使用深的卷积神经网络，在图像分类比赛中取得了非常好的成绩。
- AlexNet提出了relu函数，取代了传统的sigmoid函数。
- 

## VGGNet

## GoogleNet

## ResNet
- ResNet解决了梯度消失的问题

## DenseNet

## 迁移学习
- 迁移学习是指，先在源数据集上进行预训练，然后在特定的数据集上进行微调。
- 在微调时，backbone不需要重新训练，但是输出层（head）需要重新进行训练。

# 目标检测任务

## 传统的目标检测方法

## OverFeat模型

## RCNN

## SPP-Net

## Fast RCNN

## YOLO

## SSD


# 图像分割

## FCN

## SegNet

## Mask RCNN

## U-Nets

## PSPNet

## transformer VIT

# 序列数据处理与自然语言处理

## RNN
RNN是处理序列数据的基本网络  
代码如下：
```py
class SimpleRNN:
    def __init__(self, hidden_size, input_size):
        self.Whh = np.random.randn(hidden_size, hidden_size)  # 隐藏层到隐藏层的权重
        self.Wxh = np.random.randn(hidden_size, input_size)   # 输入到隐藏层的权重
        self.bh = np.zeros((hidden_size, 1))                  # 隐藏层偏置
        
    def forward(self, x, h_prev):
        # x: 当前输入
        # h_prev: 上一时刻的隐藏状态
        h_next = np.tanh(np.dot(self.Whh, h_prev) + np.dot(self.Wxh, x) + self.bh)
        return h_next
```
- RNN每个时刻的输出等于 当前输入+上一个时刻的输出。并乘上一个可学习的参数，并加上一个可以学习的偏置值。
- RNN的短记忆和梯度消失问题：对于一个长度为L的序列，如果将其放入到RNN中，RNN的每一个神经元都需要计算L个token，生成的计算图深度大致为L。对于长度较大的序列，会导致计算图过深进而梯度消失。梯度消失问题会使得RNN不能建立起长距离的依赖关系，这就是RNN的短记忆问题。

## 长短时记忆网络（LSTM）
```py
import numpy as np

class LSTM:
    def __init__(self, input_size, hidden_size):
        # 初始化权重矩阵
        self.Wf = np.random.randn(hidden_size, hidden_size + input_size)  # 遗忘门权重
        self.Wi = np.random.randn(hidden_size, hidden_size + input_size)  # 输入门权重
        self.Wc = np.random.randn(hidden_size, hidden_size + input_size)  # 候选状态权重
        self.Wo = np.random.randn(hidden_size, hidden_size + input_size)  # 输出门权重
        
        # 初始化偏置项
        self.bf = np.zeros((hidden_size, 1))
        self.bi = np.zeros((hidden_size, 1))
        self.bc = np.zeros((hidden_size, 1))
        self.bo = np.zeros((hidden_size, 1))
    
    def forward(self, x, h_prev, c_prev):
        # 连接输入和前一个隐藏状态
        combined = np.vstack((h_prev, x))
        
        # 1. 遗忘门
        f = self.sigmoid(np.dot(self.Wf, combined) + self.bf)
        
        # 2. 输入门
        i = self.sigmoid(np.dot(self.Wi, combined) + self.bi)
        
        # 3. 候选状态
        c_tilde = np.tanh(np.dot(self.Wc, combined) + self.bc)
        
        # 4. 更新记忆单元
        c_next = f * c_prev + i * c_tilde
        
        # 5. 输出门
        o = self.sigmoid(np.dot(self.Wo, combined) + self.bo)
        
        # 6. 计算新的隐藏状态
        h_next = o * np.tanh(c_next)
        
        return h_next, c_next
    
    @staticmethod
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))
```
LSTM的思想是将序列分为两支，一支是关键记忆，另一支是临时记忆。

1. 当一个新的token到来时，首先将其拼接到短期记忆h上，得到合并结果combine  
2. 之后将combine放入到forget layer中，得到需要忘记的下标  
3. 之后将combine放入到candidate layer中，得到需要放入到长期记忆中的下标
4. 之后将合并之后的combine通过神经网络得到input向量（由于前面是token拼接到上面的，所以需要经过神经网络）
5. 将以前的c乘上f，进行遗忘，将input乘上新的candidate得到加入的信息
6. 将combine送入forget layer得到输出向量

## Seq2Seq结构
- Seq2Seq结构一般用于自然语言的翻译
- Seq2Seq结构包含编码器和解码器。在训练阶段，使用teaching force方法，就是从编码器输入需要翻译的语言，同时，从解码器输入翻译完成的语言，通过计算误差并反向传播，达到训练的目的。
- 在推理阶段，直接输入要翻译的语言即可。

## TCN

## Transformer
![](./深度学习/transformer.png)
transformer是上述的编码器解码器结构
- 注意力机制
注意力机制的公式如下
$Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$
KQ相乘可以计算出一个相似度矩阵（注意力权重），表示每个向量和其他向量的相似度。如果将V乘上这个矩阵，就会给V的不同部分赋予不同的权重，从而实现注意力机制。
softmax是为了将权重归一化成得分
dk是k向量的维度（qk的维度是相同的）。除以dk是为了防止QK矩阵方差过大的情况发生。
- 基于teaching force的训练
在训练时，右侧会输入一个真实值ground truth，在训练时，来进行监督学习。右侧的第一个attention块上有一个mask，在进行监督学习时，模型逐个推理出token，mask会将后面的token遮盖，只留下前面的token来进行监督学习。
- decoder中的两个attention块
第一个attention是自注意力（self attention），第二个attention是crosss attention，是将decoder的查询(query)与encoder的输出(key,value)进行attention计算。

# 自然语言处理
## 分词
分词包括三种方法，分别是
- 基于字典的分词方法
- 基于理解的分词方法
- 基于统计的分词方法

## 词性标注

## 依存结构

## 文本的向量化表达
- 独热编码
- 词袋模型

## TF-IDF
字词的重要性和词的出现频率一般正相关
- TF:词频
一般词频高的词有更高的重要性
- IDF:逆向文本频率
一个词如果在所有的文档中都出现，则这个词含有的信息量较少
- TF-IDF=TF*IDF



# 图神经网络
- 图神经网络是一类专门处理图结构数据的深度学习模型。
- 图神经网络主要有两种处理方式，分别是空间方法和谱方法。
- 图由节点和边构成。在实际任务中，有的任务需要学习和预测节点，这类任务称为节点级任务。有的任务需要学习和预测边，这些任务称为边缘级任务。
## 空间方法
![](./深度学习/simple_GNN.png)
- 最简单的GNN中，有多层的神经网络，每层神经网络的结构如上图所示。
将图的边E和节点V提取出来，然后通过一个变换函数f，得到下一层的结果。这种方法不会改变图的结构。
- 空间方法的pooling
如果我们想要得到一个预测值，必须将整个图网络映射到一个标量上去。  
类似于CNN的pooling，对于节点上的信息，GNN的pooling是将相邻节点和当前节点的信息求和，之后取平均值（avg pooling）。相似地，也有最大值pooling。
对于边的pooling，是将所有相邻边的特征向量加和，然后进行pooling。
- 在简单的GNN网络中，不能建模节点和节点之间的交互关系，也不能建模出节点和边的交互关系。
- 为了建立节点和边之间的交互关系，我们将节点的特征和边的特征进行交互。
![](./深度学习/GNN_edge_node_communication.png)
- 为了提取出全局特征，我们可以专门设置一个全局特征向量，来与节点和边进行交互。如下图所示，网络通过映射将节点的特征和边的特征汇聚到全局特征U上。
![](./深度学习/GNN_global_feature.png)


## 谱方法
谱方法的流程：
构建拉普拉斯矩阵、拉普拉斯矩阵谱分解、傅立叶变换、图上的傅立叶变换、图卷积公式

# 生成对抗神经网络

